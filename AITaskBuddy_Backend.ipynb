{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zmSvunIBOcU",
        "outputId": "417d8fba-3c1d-4aab-f445-41e6f9a6b64b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: azure.identity in /usr/local/lib/python3.11/dist-packages (1.21.0)\n",
            "Requirement already satisfied: azure-core>=1.31.0 in /usr/local/lib/python3.11/dist-packages (from azure.identity) (1.32.0)\n",
            "Requirement already satisfied: cryptography>=2.5 in /usr/local/lib/python3.11/dist-packages (from azure.identity) (43.0.3)\n",
            "Requirement already satisfied: msal>=1.30.0 in /usr/local/lib/python3.11/dist-packages (from azure.identity) (1.32.0)\n",
            "Requirement already satisfied: msal-extensions>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from azure.identity) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from azure.identity) (4.12.2)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.31.0->azure.identity) (2.32.3)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.31.0->azure.identity) (1.17.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=2.5->azure.identity) (1.17.1)\n",
            "Requirement already satisfied: PyJWT<3,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure.identity) (2.10.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=2.5->azure.identity) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.31.0->azure.identity) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.31.0->azure.identity) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.31.0->azure.identity) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.31.0->azure.identity) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "pip install azure.identity"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.inference import ChatCompletionsClient\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "# For Serverless API or Managed Compute endpoints\n",
        "client = ChatCompletionsClient(\n",
        "    endpoint=\"https://slung-m8gmpzgx-eastus2.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-10-21\",\n",
        "    credential=AzureKeyCredential(\"AITUMNeTWXjLocL36IgmqlQLysFAlL8HXaojy4FTjJVpi4nFzDflJQQJ99BCACHYHv6XJ3w3AAAAACOG2RAY\")\n",
        ")"
      ],
      "metadata": {
        "id": "dw-D_rCpBVnT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install azure-ai-inference"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "UCfdo3FqBZV0",
        "outputId": "28f94d61-9d75-4755-bef2-cf8b42ce4bd5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting azure-ai-inference\n",
            "  Downloading azure_ai_inference-1.0.0b9-py3-none-any.whl.metadata (34 kB)\n",
            "Collecting isodate>=0.6.1 (from azure-ai-inference)\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: azure-core>=1.30.0 in /usr/local/lib/python3.11/dist-packages (from azure-ai-inference) (1.32.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from azure-ai-inference) (4.12.2)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.30.0->azure-ai-inference) (2.32.3)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.30.0->azure-ai-inference) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-inference) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-inference) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-inference) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-inference) (2025.1.31)\n",
            "Downloading azure_ai_inference-1.0.0b9-py3-none-any.whl (124 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.9/124.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: isodate, azure-ai-inference\n",
            "Successfully installed azure-ai-inference-1.0.0b9 isodate-0.7.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "azure"
                ]
              },
              "id": "d2d3d30d0f0c406c8590a5f8d0b38772"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from azure.ai.inference import ChatCompletionsClient\n",
        "from azure.ai.inference.models import SystemMessage, UserMessage\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "endpoint = \"https://slung-m8gmpzgx-eastus2.openai.azure.com/openai/deployments/gpt-4\"\n",
        "model_name = \"gpt-4\"\n",
        "\n",
        "client = ChatCompletionsClient(\n",
        "    endpoint=endpoint,\n",
        "    credential=AzureKeyCredential(\"AITUMNeTWXjLocL36IgmqlQLysFAlL8HXaojy4FTjJVpi4nFzDflJQQJ99BCACHYHv6XJ3w3AAAAACOG2RAY\"),\n",
        ")\n",
        "\n",
        "response = client.complete(\n",
        "    messages=[\n",
        "        SystemMessage(content=\"You are a helpful assistant working with people with accessibility needs.\"),\n",
        "        UserMessage(content=\"help me break down this task, writing an email?\")\n",
        "    ],\n",
        "    max_tokens=400,\n",
        "    temperature=1.0,\n",
        "    top_p=1.0,\n",
        "    model=model_name\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7mORlnvBuBy",
        "outputId": "bab4460f-e369-4d1a-a744-7790ffd5f01f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Certainly! Writing an effective email consists of several key components. Breaking down the process can help you write better and more purposeful emails. Here’s how you can break down the task:\n",
            "\n",
            "### 1. Define the Purpose\n",
            "Understand why you are writing the email. Each email should have a clear objective such as to inform, request, thank, confirm, update, etc.\n",
            "\n",
            "### 2. Know Your Audience\n",
            "Identify who you are writing to: a colleague, supervisor, client, or a wider audience. This will affect the tone and content of your email.\n",
            "\n",
            "### 3. Create a Clear Subject Line\n",
            "Write a concise subject line that reflects the content of the email. This helps the recipient understand the importance of the email and whether it needs urgent attention.\n",
            "\n",
            "### 4. Greeting\n",
            "Choose an appropriate greeting:\n",
            "- Formal: \"Dear [Name],\"\n",
            "- Less formal: \"Hello [Name],\"\n",
            "- Group: \"Hi everyone,\"\n",
            "\n",
            "### 5. Write the Body\n",
            "- **Opening Line:** Start with a polite opening, such as thanking the recipient if appropriate, or stating the purpose of your email.\n",
            "- **Main Content:** Clearly state your main point or request in the initial sentences. Provide necessary details but keep it concise. Use bullet points if helpful.\n",
            "- **Purposeful Closing:** End the main content by reiterating the purpose if necessary, or stating what action you hope the recipient will take.\n",
            "\n",
            "### 6. Closing Line\n",
            "Finish with a polite closing line, thanking the reader for their attention, or expressing hopes for a response. Example: \"Thank you for your attention to this matter.\"\n",
            "\n",
            "### 7. Sign-Off\n",
            "Choose an appropriate sign-off:\n",
            "- Formal: \"Best regards,\" \"Sincerely,\"\n",
            "- Informal: \"Best,\" \"Thanks,\"\n",
            "\n",
            "Include your full name and additional contact information if it's the first communication or if contextually appropriate.\n",
            "\n",
            "### 8. Attachment (if any)\n",
            "If you have included any\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import os\n",
        "import time\n",
        "from azure.ai.inference import ChatCompletionsClient\n",
        "from azure.ai.inference.models import SystemMessage, UserMessage\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from azure.core.exceptions import HttpResponseError\n",
        "\n",
        "endpoint = \"https://slung-m8gmpzgx-eastus2.openai.azure.com/openai/deployments/gpt-4\"\n",
        "model_name = \"gpt-4\"\n",
        "\n",
        "client = ChatCompletionsClient(\n",
        "    endpoint=endpoint,\n",
        "    credential=AzureKeyCredential(\"AITUMNeTWXjLocL36IgmqlQLysFAlL8HXaojy4FTjJVpi4nFzDflJQQJ99BCACHYHv6XJ3w3AAAAACOG2RAY\"),\n",
        ")\n",
        "\n",
        "# Retry mechanism with exponential backoff\n",
        "retries = 3  # Number of retries\n",
        "delay = 5    # Initial delay in seconds\n",
        "\n",
        "for i in range(retries):\n",
        "    try:\n",
        "        response = client.complete(\n",
        "            messages=[\n",
        "                SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "                UserMessage(content=\"I am going to Paris, what should I see?\")\n",
        "            ],\n",
        "            max_tokens=400,\n",
        "            temperature=1.0,\n",
        "            top_p=1.0,\n",
        "            model=model_name\n",
        "        )\n",
        "        print(response.choices[0].message.content)\n",
        "        break  # Exit loop if successful\n",
        "    except HttpResponseError as e:\n",
        "        if e.status_code == 429:  # Check for rate limit error\n",
        "            print(f\"Rate limit exceeded. Retrying in {delay} seconds...\")\n",
        "            time.sleep(delay)\n",
        "            delay *= 2  # Exponential backoff\n",
        "        else:\n",
        "            raise  # Re-raise other exceptions"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2qmanpjHRYS",
        "outputId": "bfd155ef-f7d7-45e6-d7e3-f3ba82251209"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paris is filled with iconic attractions, fascinating history, and charming spots. Here are some must-see sights and experiences:\n",
            "\n",
            "1. **Eiffel Tower**: The most emblematic symbol of Paris. Either day or night, a visit to the Eiffel Tower is a must. For a special experience, have dinner at one of its restaurants.\n",
            "\n",
            "2. **Louvre Museum**: Home to thousands of works of art, including the Mona Lisa and the Venus de Milo. Allocate enough time as the museum is vast.\n",
            "\n",
            "3. **Cathédrale Notre-Dame de Paris**: A magnificent example of French Gothic architecture. Although it was damaged by fire in 2019, you can still admire the structure from the outside.\n",
            "\n",
            "4. **Montmartre**: Wander around this bohemian hilltop neighborhood, visit the Basilica of the Sacré-Cœur, and enjoy panoramic views of Paris.\n",
            "\n",
            "5. **Seine River Cruise**: A cruise on the Seine offers a great way to see many of Paris's key sights like the Louvre, the Eiffel Tower, and Notre-Dame along its banks.\n",
            "\n",
            "6. **Arc de Triomphe**: Situated at the western end of the Champs-Élysées, this massive arch commemorates those who fought and died for France. You can climb to the top for amazing views.\n",
            "\n",
            "7. **Champs-Élysées**: One of the most famous avenues in the world, great for a stroll to take in the Parisian atmosphere.\n",
            "\n",
            "8. **Musée d'Orsay**: Housing the largest collection of impressionist and post-Impressionist masterpieces in the world, it's a must for art lovers.\n",
            "\n",
            "9. **Palace of Versailles**: A short train ride from Paris, this opulent palace and its grand gardens are well worth a visit.\n",
            "\n",
            "10. **Latin Quarter**: Explore this historic area known for its vibrant student life, bistros, and landmarks like the Panthéon\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from azure.ai.inference import ChatCompletionsClient\n",
        "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "endpoint = \"https://slung-m8gmpzgx-eastus2.openai.azure.com/openai/deployments/gpt-4\"\n",
        "model_name = \"gpt-4\"\n",
        "\n",
        "client = ChatCompletionsClient(\n",
        "    endpoint=endpoint,\n",
        "    credential=AzureKeyCredential(\"AITUMNeTWXjLocL36IgmqlQLysFAlL8HXaojy4FTjJVpi4nFzDflJQQJ99BCACHYHv6XJ3w3AAAAACOG2RAY\"),\n",
        ")\n",
        "\n",
        "response = client.complete(\n",
        "    messages=[\n",
        "        SystemMessage(content=\"You are a helpful assistant who does not get persuaded easily\"),\n",
        "        UserMessage(content=\"team me how to properly tell the difference between criticism, praise and feedback, and reacting appropriately\"),\n",
        "\n",
        "    ],\n",
        "    max_tokens=400,\n",
        "    temperature=1.0,\n",
        "    top_p=1.0,\n",
        "    model=model_name\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6Qi2KeQDUMt",
        "outputId": "4690c03d-92d2-4dd0-82ad-377b3862c08d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To effectively differentiate and respond appropriately to criticism, praise, and feedback, consider the following approaches:\n",
            "\n",
            "### 1. Understanding the Differences\n",
            "- **Criticism:** Often points out what is wrong or what needs improvement and may not always be constructive. It can sometimes be presented in a negative or harsh tone.\n",
            "- **Praise:** Recognition or approval of what someone has done well. It focuses on success and positive outcomes.\n",
            "- **Feedback:** Typically involves specific information about how something is viewed by others. Feedback can include both positive and negative aspects, but it is generally aimed at offering insights to support improvement or growth. It is more balanced and detailed than mere criticism or praise.\n",
            "\n",
            "### 2. Reacting Appropriately\n",
            "\n",
            "#### To Criticism:\n",
            "- **Listen Carefully:** Try to listen without becoming defensive. Understand what the other person is saying and why they feel that way.\n",
            "- **Seek Clarification:** If something is unclear, ask specific questions to get more detailed insights about the criticism.\n",
            "- **Evaluate:** Consider whether the criticism is constructive and if it can be used to make positive changes.\n",
            "- **Respond Politely:** Whether you agree with the criticism or not, respond respectfully and calmly. You can either thank the critic for their input or explain your perspective without escalating the situation.\n",
            "\n",
            "#### To Praise:\n",
            "- **Acknowledge Graciously:** A simple “thank you” is often the best response when receiving praise. It shows that you appreciate the recognition.\n",
            "- **Express Gratitude:** You might also express gratitude for the specific effort recognized, especially if it was a team effort.\n",
            "- **Stay Humble and Motivated:** Use praise as motivation to continue performing well and improving further.\n",
            "\n",
            "#### To Feedback:\n",
            "- **Listen Actively:** Engage with the feedback provider, showing genuine interest in their comments. Listening actively means not planning what to say next while the other person is speaking.\n",
            "- **Discuss and Ask for Examples:** To better understand and to make the feedback actionable, ask for\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I am testing out document summarization!"
      ],
      "metadata": {
        "id": "SIIWJYysEyuJ"
      }
    },
    {
      "source": [
        "!pip install azure-ai-textanalytics"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "tVJtlh-LFiI0",
        "outputId": "9bd0339e-9dee-40d2-9285-905278fe5d34"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting azure-ai-textanalytics\n",
            "  Downloading azure_ai_textanalytics-5.3.0-py3-none-any.whl.metadata (82 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/82.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: azure-core<2.0.0,>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from azure-ai-textanalytics) (1.32.0)\n",
            "Collecting azure-common~=1.1 (from azure-ai-textanalytics)\n",
            "  Downloading azure_common-1.1.28-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: isodate<1.0.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from azure-ai-textanalytics) (0.7.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from azure-ai-textanalytics) (4.12.2)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2.32.3)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2025.1.31)\n",
            "Downloading azure_ai_textanalytics-5.3.0-py3-none-any.whl (298 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.6/298.6 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
            "Installing collected packages: azure-common, azure-ai-textanalytics\n",
            "Successfully installed azure-ai-textanalytics-5.3.0 azure-common-1.1.28\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "azure"
                ]
              },
              "id": "962fa248323f4d509e0000df0a707be1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# This example requires environment variables named \"AZURE_AI_KEY\" and \"ENDPOINT_TO_CALL_LANGUAGE_API\"\n",
        "key = \"4NVVBgTDzN81KPooWwjez6ItMgbWaGBjyZpAAdhFGTJtkfPsm1DGJQQJ99BCACYeBjFXJ3w3AAAAACOGASAx\" # Replace with your actual key\n",
        "endpoint = \"https://aitaskbuddy5634585164.cognitiveservices.azure.com/\" # Replace with your actual endpoint\n",
        "\n",
        "from azure.ai.textanalytics import TextAnalyticsClient\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "# Authenticate the client using your key and endpoint\n",
        "def authenticate_client():\n",
        "    ta_credential = AzureKeyCredential(key)\n",
        "    text_analytics_client = TextAnalyticsClient(\n",
        "            endpoint=endpoint,\n",
        "            credential=ta_credential)\n",
        "    return text_analytics_client\n",
        "\n",
        "client = authenticate_client()\n",
        "\n",
        "# Example method for summarizing text\n",
        "def sample_extractive_summarization(client):\n",
        "    from azure.core.credentials import AzureKeyCredential\n",
        "    from azure.ai.textanalytics import (\n",
        "        TextAnalyticsClient,\n",
        "        ExtractiveSummaryAction\n",
        "    )\n",
        "\n",
        "    document = [\n",
        "        \"Artificial intelligence is transforming the healthcare industry by improving diagnostics, automating administrative tasks, and personalizing patient treatment.\"\n",
        "        \"Machine learning models can analyze vast amounts of medical data to detect diseases earlier and with greater accuracy. Additionally, \"\n",
        "        \"AI-powered chatbots and virtual assistants help streamline patient interactions, reducing the workload on healthcare professionals. \"\n",
        "        \"However, challenges such as data privacy concerns and the need for regulatory compliance remain critical in the widespread adoption of AI in healthcare.\"\n",
        "\n",
        "    ]\n",
        "\n",
        "    poller = client.begin_analyze_actions(\n",
        "        document,\n",
        "        actions=[\n",
        "            ExtractiveSummaryAction(max_sentence_count=4)\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    document_results = poller.result()\n",
        "    for result in document_results:\n",
        "        extract_summary_result = result[0]  # first document, first result\n",
        "        if extract_summary_result.is_error:\n",
        "            print(\"...Is an error with code '{}' and message '{}'\".format(\n",
        "                extract_summary_result.code, extract_summary_result.message\n",
        "            ))\n",
        "        else:\n",
        "            print(\"Summary extracted: {}\".format(\n",
        "                \" \".join([sentence.text for sentence in extract_summary_result.sentences])))\n",
        "\n",
        "sample_extractive_summarization(client)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTgeIhMuE25m",
        "outputId": "04cd8b00-f037-4ab9-fff7-b634a479b659"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary extracted: Artificial intelligence is transforming the healthcare industry by improving diagnostics, automating administrative tasks, and personalizing patient treatment. Machine learning models can analyze vast amounts of medical data to detect diseases earlier and with greater accuracy. Additionally, AI-powered chatbots and virtual assistants help streamline patient interactions, reducing the workload on healthcare professionals. However, challenges such as data privacy concerns and the need for regulatory compliance remain critical in the widespread adoption of AI in healthcare.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing out Speech Capabilities\n"
      ],
      "metadata": {
        "id": "eW1nuNnbKgBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install azure-cognitiveservices-speech"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPqEHL8uMlmJ",
        "outputId": "3cefe445-ca46-4961-8ad9-247c8d9beb08"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting azure-cognitiveservices-speech\n",
            "  Downloading azure_cognitiveservices_speech-1.43.0-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Downloading azure_cognitiveservices_speech-1.43.0-py3-none-manylinux1_x86_64.whl (40.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: azure-cognitiveservices-speech\n",
            "Successfully installed azure-cognitiveservices-speech-1.43.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key = \"4NVVBgTDzN81KPooWwjez6ItMgbWaGBjyZpAAdhFGTJtkfPsm1DGJQQJ99BCACYeBjFXJ3w3AAAAACOGASAx\"\n",
        "endpoint = \"https://aitaskbuddy5634585164.cognitiveservices.azure.com/\""
      ],
      "metadata": {
        "id": "s2wK_EcAKjfR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"SPEECH_KEY\"] = \"4NVVBgTDzN81KPooWwjez6ItMgbWaGBjyZpAAdhFGTJtkfPsm1DGJQQJ99BCACYeBjFXJ3w3AAAAACOGASAx\"\n",
        "os.environ[\"SPEECH_REGION\"] = \"eastus\"\n"
      ],
      "metadata": {
        "id": "i_zSYRiaNW1g"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TESTING OUT SPEECH FOR A WAV FILE"
      ],
      "metadata": {
        "id": "RCzebXCpQMGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# This will open a file upload dialog.\n",
        "uploaded_files = files.upload()\n",
        "\n",
        "# List the uploaded files\n",
        "for filename in uploaded_files.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "        name=filename, length=len(uploaded_files[filename])))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "sH9EI6SOPU2d",
        "outputId": "ba3ca334-e8bd-479b-ab6e-2a0c6e9b2b43"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b5bffcfd-3ea9-4365-8c58-61342024ec7d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b5bffcfd-3ea9-4365-8c58-61342024ec7d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving cietta 4.m4a to cietta 4.m4a\n",
            "User uploaded file \"cietta 4.m4a\" with length 37546 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install ffmpeg -y\n",
        "!ffmpeg -i \"cietta 4.m4a\" -ar 16000 -ac 1 output.wav\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EE2yL46P8R8",
        "outputId": "0158558d-1077-4331-c055-d670996ccdcc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n",
            "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'cietta 4.m4a':\n",
            "  Metadata:\n",
            "    major_brand     : M4A \n",
            "    minor_version   : 0\n",
            "    compatible_brands: M4A isommp42\n",
            "    creation_time   : 2023-09-25T15:22:14.000000Z\n",
            "    title           : cietta 4\n",
            "    voice-memo-uuid : 613A9928-77A6-421E-B0E8-09BAAEC97BB1\n",
            "    encoder         : com.apple.VoiceMemos (iPhone Version 16.3.1 (Build 20D67))\n",
            "  Duration: 00:00:03.90, start: 0.000000, bitrate: 77 kb/s\n",
            "  Stream #0:0(und): Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, mono, fltp, 65 kb/s (default)\n",
            "    Metadata:\n",
            "      creation_time   : 2023-09-25T15:22:14.000000Z\n",
            "      handler_name    : Core Media Audio\n",
            "      vendor_id       : [0][0][0][0]\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (aac (native) -> pcm_s16le (native))\n",
            "Press [q] to stop, [?] for help\n",
            "Output #0, wav, to 'output.wav':\n",
            "  Metadata:\n",
            "    major_brand     : M4A \n",
            "    minor_version   : 0\n",
            "    compatible_brands: M4A isommp42\n",
            "    voice-memo-uuid : 613A9928-77A6-421E-B0E8-09BAAEC97BB1\n",
            "    INAM            : cietta 4\n",
            "    ISFT            : Lavf58.76.100\n",
            "  Stream #0:0(und): Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s (default)\n",
            "    Metadata:\n",
            "      creation_time   : 2023-09-25T15:22:14.000000Z\n",
            "      handler_name    : Core Media Audio\n",
            "      vendor_id       : [0][0][0][0]\n",
            "      encoder         : Lavc58.134.100 pcm_s16le\n",
            "size=     123kB time=00:00:03.94 bitrate= 256.3kbits/s speed= 197x    \n",
            "video:0kB audio:123kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.076013%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import azure.cognitiveservices.speech as speechsdk\n",
        "\n",
        "def recognize_from_audio_file(audio_filename):\n",
        "    # Ensure your environment variables are set for Speech Key and Region\n",
        "    speech_config = speechsdk.SpeechConfig(subscription=os.environ.get('SPEECH_KEY'),\n",
        "                                           region=os.environ.get('SPEECH_REGION'))\n",
        "    speech_config.speech_recognition_language = \"en-US\"\n",
        "\n",
        "    # Use the uploaded audio file instead of the microphone\n",
        "    audio_config = speechsdk.audio.AudioConfig(filename=audio_filename)\n",
        "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
        "\n",
        "    print(\"Recognizing speech from audio file...\")\n",
        "    speech_recognition_result = speech_recognizer.recognize_once_async().get()\n",
        "\n",
        "    if speech_recognition_result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
        "        print(\"Recognized: {}\".format(speech_recognition_result.text))\n",
        "    elif speech_recognition_result.reason == speechsdk.ResultReason.NoMatch:\n",
        "        print(\"No speech could be recognized: {}\".format(speech_recognition_result.no_match_details))\n",
        "    elif speech_recognition_result.reason == speechsdk.ResultReason.Canceled:\n",
        "        cancellation_details = speech_recognition_result.cancellation_details\n",
        "        print(\"Speech Recognition canceled: {}\".format(cancellation_details.reason))\n",
        "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
        "            print(\"Error details: {}\".format(cancellation_details.error_details))\n",
        "            print(\"Did you set the speech resource key and region values?\")\n",
        "\n",
        "# Replace the filename below with the name of your uploaded file.\n",
        "audio_filename = \"output.wav\"\n",
        "recognize_from_audio_file(audio_filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WD7v-PaPq9V",
        "outputId": "74f570de-ad04-42ff-ee3f-d414d5b6ce9f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recognizing speech from audio file...\n",
            "Recognized: However, things turned around for her when she started getting more involved on campus.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TESTING OUT SPEECH FOR A MICROPHONE\n",
        "\n",
        "*   The code curently gives an error because i am running colab online and it does not have access to a microphone\n",
        "\n"
      ],
      "metadata": {
        "id": "GsMY1JoiQT4n"
      }
    },
    {
      "source": [
        "import os\n",
        "import azure.cognitiveservices.speech as speechsdk\n",
        "\n",
        "def recognize_from_microphone():\n",
        "    # This example requires environment variables named \"SPEECH_KEY\" and \"SPEECH_REGION\"\n",
        "    speech_config = speechsdk.SpeechConfig(subscription=os.environ.get('SPEECH_KEY'), region=os.environ.get('SPEECH_REGION'))\n",
        "    speech_config.speech_recognition_language=\"en-US\"\n",
        "\n",
        "    audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
        "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
        "\n",
        "    print(\"Speak into your microphone.\")\n",
        "    speech_recognition_result = speech_recognizer.recognize_once_async().get()\n",
        "\n",
        "    if speech_recognition_result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
        "        print(\"Recognized: {}\".format(speech_recognition_result.text))\n",
        "    elif speech_recognition_result.reason == speechsdk.ResultReason.NoMatch:\n",
        "        print(\"No speech could be recognized: {}\".format(speech_recognition_result.no_match_details))\n",
        "    elif speech_recognition_result.reason == speechsdk.ResultReason.Canceled:\n",
        "        cancellation_details = speech_recognition_result.cancellation_details\n",
        "        print(\"Speech Recognition canceled: {}\".format(cancellation_details.reason))\n",
        "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
        "            print(\"Error details: {}\".format(cancellation_details.error_details))\n",
        "            print(\"Did you set the speech resource key and region values?\")\n",
        "\n",
        "recognize_from_microphone()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-61MLSm7W8l6",
        "outputId": "21069782-386e-4ca6-b5c3-9be413786a1d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Exception with error code: \n[CALL STACK BEGIN]\n\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.extension.audio.sys.so(+0x10168) [0x7daea0034168]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0xa75e6) [0x7dae8a2b95e6]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1898fb) [0x7dae8a39b8fb]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0xe2650) [0x7dae8a2f4650]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1940f2) [0x7dae8a3a60f2]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0xa75e6) [0x7dae8a2b95e6]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1898fb) [0x7dae8a39b8fb]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0xe02dd) [0x7dae8a2f22dd]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0xe393b) [0x7dae8a2f593b]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x149fcb) [0x7dae8a35bfcb]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x149fcb) [0x7dae8a35bfcb]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x10a663) [0x7dae8a31c663]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x13d5f6) [0x7dae8a34f5f6]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x99557) [0x7dae8a2ab557]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x13cb2b) [0x7dae8a34eb2b]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x91e35) [0x7dae8a2a3e35]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(recognizer_create_speech_recognizer_from_config+0x114) [0x7dae8a3e8953]\n[CALL STACK END]\n\nException with an error code: 0xe (SPXERR_MIC_NOT_AVAILABLE)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-8cb8e1b73f1f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Did you set the speech resource key and region values?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mrecognize_from_microphone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-8cb8e1b73f1f>\u001b[0m in \u001b[0;36mrecognize_from_microphone\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0maudio_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspeechsdk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAudioConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_default_microphone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mspeech_recognizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspeechsdk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpeechRecognizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeech_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspeech_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maudio_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Speak into your microphone.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/speech.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, speech_config, audio_config, language, source_language_config, auto_detect_source_language_config)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0maudio_config_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0maudio_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msource_language_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mauto_detect_source_language_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             _call_hr_fn(\n\u001b[0m\u001b[1;32m   1008\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_sdk_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecognizer_create_speech_recognizer_from_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m                 *[ctypes.byref(handle), speech_config._handle, audio_config_handle])\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/interop.py\u001b[0m in \u001b[0;36m_call_hr_fn\u001b[0;34m(fn, *args)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_spx_hr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mhr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0m_raise_if_failed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/interop.py\u001b[0m in \u001b[0;36m_raise_if_failed\u001b[0;34m(hr)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_if_failed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_spx_hr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhr\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0m__try_get_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_spx_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/interop.py\u001b[0m in \u001b[0;36m__try_get_error\u001b[0;34m(error_handle)\u001b[0m\n\u001b[1;32m     48\u001b[0m     )\n\u001b[1;32m     49\u001b[0m     \u001b[0m_sdk_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_release\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Exception with error code: \n[CALL STACK BEGIN]\n\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.extension.audio.sys.so(+0x10168) [0x7daea0034168]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0xa75e6) [0x7dae8a2b95e6]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1898fb) [0x7dae8a39b8fb]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0xe2650) [0x7dae8a2f4650]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1940f2) [0x7dae8a3a60f2]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0xa75e6) [0x7dae8a2b95e6]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1898fb) [0x7dae8a39b8fb]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0xe02dd) [0x7dae8a2f22dd]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0xe393b) [0x7dae8a2f593b]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x149fcb) [0x7dae8a35bfcb]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x149fcb) [0x7dae8a35bfcb]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x10a663) [0x7dae8a31c663]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x13d5f6) [0x7dae8a34f5f6]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x99557) [0x7dae8a2ab557]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x13cb2b) [0x7dae8a34eb2b]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x91e35) [0x7dae8a2a3e35]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(recognizer_create_speech_recognizer_from_config+0x114) [0x7dae8a3e8953]\n[CALL STACK END]\n\nException with an error code: 0xe (SPXERR_MIC_NOT_AVAILABLE)"
          ]
        }
      ]
    }
  ]
}