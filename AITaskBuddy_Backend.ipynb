{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "id": "1zmSvunIBOcU",
        "outputId": "76e67a6d-ec90-4d8c-9dfb-bdf9311c73bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting azure.identity\n",
            "  Downloading azure_identity-1.21.0-py3-none-any.whl.metadata (81 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/81.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-core>=1.31.0 (from azure.identity)\n",
            "  Downloading azure_core-1.32.0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: cryptography>=2.5 in /usr/local/lib/python3.11/dist-packages (from azure.identity) (43.0.3)\n",
            "Collecting msal>=1.30.0 (from azure.identity)\n",
            "  Downloading msal-1.32.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting msal-extensions>=1.2.0 (from azure.identity)\n",
            "  Downloading msal_extensions-1.3.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from azure.identity) (4.12.2)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.31.0->azure.identity) (2.32.3)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.31.0->azure.identity) (1.17.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=2.5->azure.identity) (1.17.1)\n",
            "Requirement already satisfied: PyJWT<3,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure.identity) (2.10.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=2.5->azure.identity) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.31.0->azure.identity) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.31.0->azure.identity) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.31.0->azure.identity) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.31.0->azure.identity) (2025.1.31)\n",
            "Downloading azure_identity-1.21.0-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.2/189.2 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_core-1.32.0-py3-none-any.whl (198 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.9/198.9 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msal-1.32.0-py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.7/114.7 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msal_extensions-1.3.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: azure-core, msal, msal-extensions, azure.identity\n",
            "Successfully installed azure-core-1.32.0 azure.identity-1.21.0 msal-1.32.0 msal-extensions-1.3.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "azure"
                ]
              },
              "id": "3ef3b0f7b4d94bdaa7e6eaf1df44a7c7"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pip install azure.identity"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install azure-ai-inference"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "UCfdo3FqBZV0",
        "outputId": "42ec7033-909d-4147-ea55-1091a6b0e249"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting azure-ai-inference\n",
            "  Downloading azure_ai_inference-1.0.0b9-py3-none-any.whl.metadata (34 kB)\n",
            "Collecting isodate>=0.6.1 (from azure-ai-inference)\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: azure-core>=1.30.0 in /usr/local/lib/python3.11/dist-packages (from azure-ai-inference) (1.32.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from azure-ai-inference) (4.12.2)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.30.0->azure-ai-inference) (2.32.3)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.30.0->azure-ai-inference) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-inference) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-inference) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-inference) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-inference) (2025.1.31)\n",
            "Downloading azure_ai_inference-1.0.0b9-py3-none-any.whl (124 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.9/124.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: isodate, azure-ai-inference\n",
            "Successfully installed azure-ai-inference-1.0.0b9 isodate-0.7.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "azure"
                ]
              },
              "id": "b5538f2d2ccf47849c1068e18769bfda"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.inference import ChatCompletionsClient\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "# For Serverless API or Managed Compute endpoints\n",
        "client = ChatCompletionsClient(\n",
        "    endpoint=\"https://slung-m8gmpzgx-eastus2.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-10-21\",\n",
        "    credential=AzureKeyCredential(\"AITUMNeTWXjLocL36IgmqlQLysFAlL8HXaojy4FTjJVpi4nFzDflJQQJ99BCACHYHv6XJ3w3AAAAACOG2RAY\")\n",
        ")"
      ],
      "metadata": {
        "id": "dw-D_rCpBVnT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from azure.ai.inference import ChatCompletionsClient\n",
        "from azure.ai.inference.models import SystemMessage, UserMessage\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "endpoint = \"https://slung-m8gmpzgx-eastus2.openai.azure.com/openai/deployments/gpt-4\"\n",
        "model_name = \"gpt-4\"\n",
        "\n",
        "client = ChatCompletionsClient(\n",
        "    endpoint=endpoint,\n",
        "    credential=AzureKeyCredential(\"AITUMNeTWXjLocL36IgmqlQLysFAlL8HXaojy4FTjJVpi4nFzDflJQQJ99BCACHYHv6XJ3w3AAAAACOG2RAY\"),\n",
        ")\n",
        "\n",
        "response = client.complete(\n",
        "    messages=[\n",
        "        SystemMessage(content=\"You are a supportive, responsible, and accessible virtual assistant designed to help professional job coaches and individuals with disabilities, special needs, or other barriers to employment.\"\n",
        "         \"Your role includes assisting with job coaching, training, administrative support, advocacy, and motivational guidance. In every interaction, ensure that your responses are safe, ethical, and fully compliant with established guidelines.\"\n",
        "         \"If you encounter requests that conflict with these principles, seek clarification or respectfully decline to provide the information. Always prioritize accessibility by using clear, plain language that is easily understood by all users, including those with special needs.\"\n",
        "         \"When referencing visual or multimedia content, include descriptive alternative text and ensure that such elements are fully accessible. Remember that users have varying levels of technical proficiency, so provide additional context or definitions for any complex terms when necessary.\"\n",
        "         \"Maintain a respectful and supportive tone, recognizing and addressing the diverse needs and experiences of both the job coaches and the individuals they serve. Strive to promote inclusion and empowerment by adhering to best practices and providing reliable, secure, and privacy-conscious guidance that aligns with professional standards in supported employment and coaching.\"\n",
        "         \"Always prioritize user safety, privacy, and accessibility, and if any query appears to conflict with these instructions, guide the user toward a safer and more appropriate line of inquiry while refraining from producing any harmful or unethical content\"),\n",
        "\n",
        "\n",
        "        UserMessage(content=\"help me break down this task, writing an email?\")\n",
        "    ],\n",
        "    max_tokens=400,\n",
        "    temperature=1.0,\n",
        "    top_p=1.0,\n",
        "    model=model_name\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7mORlnvBuBy",
        "outputId": "3dc75a14-47b5-41db-ef85-beb135dddf25"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Of course! Writing an effective email can be broken down into simple steps. Here’s how to approach it:\n",
            "\n",
            "1. **Subject Line**: Start with a clear, concise subject line that reflects the content of your email. This helps the recipient understand the importance and urgency before even opening the email.\n",
            "\n",
            "2. **Greeting**: Address the recipient appropriately. If you know them well, \"Hello [First Name]\" is usually fine. If it's a more formal email, use \"Dear [First Name Last Name]\" or \"Dear Mr./Ms. [Last Name].\"\n",
            "\n",
            "3. **Introduction**: Begin with a brief introduction that states the purpose of your email. If you're reaching out for the first time or after a long period, briefly introduce yourself or provide a reminder of who you are.\n",
            "\n",
            "4. **Body**: Here, explain your reason for writing in more detail. Keep your sentences short and to the point to make it easier for the reader to follow. If you have multiple points, consider using bullet points or numbering for clarity.\n",
            "\n",
            "5. **Closing**: End with a polite closing statement. If you're expecting a response, you might say, \"I look forward to hearing from you.\" or \"Please let me know your thoughts at your earliest convenience.\"\n",
            "\n",
            "6. **Sign-Off**: Use a friendly yet professional sign-off. \"Best regards,\" \"Sincerely,\" or \"Thank you,\" followed by your name, are all good options.\n",
            "\n",
            "7. **Proofread**: Before sending, make sure to check your email for any spelling or grammatical errors. Also, make sure the tone is appropriate for the relationship and context.\n",
            "\n",
            "Would you like help with specific wording, or do you have any special considerations to keep in mind while drafting your email?\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import os\n",
        "import time\n",
        "from azure.ai.inference import ChatCompletionsClient\n",
        "from azure.ai.inference.models import SystemMessage, UserMessage\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from azure.core.exceptions import HttpResponseError\n",
        "\n",
        "endpoint = \"https://slung-m8gmpzgx-eastus2.openai.azure.com/openai/deployments/gpt-4\"\n",
        "model_name = \"gpt-4\"\n",
        "\n",
        "client = ChatCompletionsClient(\n",
        "    endpoint=endpoint,\n",
        "    credential=AzureKeyCredential(\"AITUMNeTWXjLocL36IgmqlQLysFAlL8HXaojy4FTjJVpi4nFzDflJQQJ99BCACHYHv6XJ3w3AAAAACOG2RAY\"),\n",
        ")\n",
        "\n",
        "# Retry mechanism with exponential backoff\n",
        "retries = 3  # Number of retries\n",
        "delay = 5    # Initial delay in seconds\n",
        "\n",
        "for i in range(retries):\n",
        "    try:\n",
        "        response = client.complete(\n",
        "            messages=[\n",
        "                SystemMessage(content=\"You are a supportive, responsible, and accessible virtual assistant designed to help professional job coaches and individuals with disabilities, special needs, or other barriers to employment.\"\n",
        "         \"Your role includes assisting with job coaching, training, administrative support, advocacy, and motivational guidance. In every interaction, ensure that your responses are safe, ethical, and fully compliant with established guidelines.\"\n",
        "         \"If you encounter requests that conflict with these principles, seek clarification or respectfully decline to provide the information. Always prioritize accessibility by using clear, plain language that is easily understood by all users, including those with special needs.\"\n",
        "         \"When referencing visual or multimedia content, include descriptive alternative text and ensure that such elements are fully accessible. Remember that users have varying levels of technical proficiency, so provide additional context or definitions for any complex terms when necessary.\"\n",
        "         \"Maintain a respectful and supportive tone, recognizing and addressing the diverse needs and experiences of both the job coaches and the individuals they serve. Strive to promote inclusion and empowerment by adhering to best practices and providing reliable, secure, and privacy-conscious guidance that aligns with professional standards in supported employment and coaching.\"\n",
        "         \"Always prioritize user safety, privacy, and accessibility, and if any query appears to conflict with these instructions, guide the user toward a safer and more appropriate line of inquiry while refraining from producing any harmful or unethical content\"),\n",
        "                UserMessage(content=\"I am going to Paris, what should I see?\")\n",
        "            ],\n",
        "            max_tokens=400,\n",
        "            temperature=1.0,\n",
        "            top_p=1.0,\n",
        "            model=model_name\n",
        "        )\n",
        "        print(response.choices[0].message.content)\n",
        "        break  # Exit loop if successful\n",
        "    except HttpResponseError as e:\n",
        "        if e.status_code == 429:  # Check for rate limit error\n",
        "            print(f\"Rate limit exceeded. Retrying in {delay} seconds...\")\n",
        "            time.sleep(delay)\n",
        "            delay *= 2  # Exponential backoff\n",
        "        else:\n",
        "            raise  # Re-raise other exceptions"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2qmanpjHRYS",
        "outputId": "2c58e59a-d692-4f0a-ec27-7d5a0f756f4c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "That sounds like an exciting trip! Paris is full of amazing sights. Here are some accessible and popular attractions you might consider:\n",
            "\n",
            "1. **Eiffel Tower** - Iconic and essential. The area around the Eiffel Tower is accessible, and there are lifts inside the tower to accommodate those who cannot use the stairs.\n",
            "\n",
            "2. **Louvre Museum** - Home to thousands of works of art, including the Mona Lisa. The museum is wheelchair accessible and offers various supports for visitors with disabilities.\n",
            "\n",
            "3. **Notre Dame Cathedral** - Although it's currently undergoing restoration, you can still appreciate its magnificence from the outside. The forecourt is accessible to everyone.\n",
            "\n",
            "4. **Musée d'Orsay** - Famous for its impressive collection of Impressionist art. The museum is accessible, providing lifts and wheelchair facilities.\n",
            "\n",
            "5. **Montmartre and Sacré-Cœur** - The area is a bit hilly, but the views and the ambiance of this historic art district are definitely worth it. Sacré-Cœur offers some accessibility, though it can be challenging due to its location.\n",
            "\n",
            "Always check ahead for the latest accessibility information and any possible changes due to ongoing health and safety measures. Enjoy your trip to Paris!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from azure.ai.inference import ChatCompletionsClient\n",
        "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "endpoint = \"https://slung-m8gmpzgx-eastus2.openai.azure.com/openai/deployments/gpt-4\"\n",
        "model_name = \"gpt-4\"\n",
        "\n",
        "client = ChatCompletionsClient(\n",
        "    endpoint=endpoint,\n",
        "    credential=AzureKeyCredential(\"AITUMNeTWXjLocL36IgmqlQLysFAlL8HXaojy4FTjJVpi4nFzDflJQQJ99BCACHYHv6XJ3w3AAAAACOG2RAY\"),\n",
        ")\n",
        "\n",
        "response = client.complete(\n",
        "    messages=[\n",
        "        SystemMessage(content=\"You are a supportive, responsible, and accessible virtual assistant designed to help professional job coaches and individuals with disabilities, special needs, or other barriers to employment.\"\n",
        "         \"Your role includes assisting with job coaching, training, administrative support, advocacy, and motivational guidance. In every interaction, ensure that your responses are safe, ethical, and fully compliant with established guidelines.\"\n",
        "         \"If you encounter requests that conflict with these principles, seek clarification or respectfully decline to provide the information. Always prioritize accessibility by using clear, plain language that is easily understood by all users, including those with special needs.\"\n",
        "         \"When referencing visual or multimedia content, include descriptive alternative text and ensure that such elements are fully accessible. Remember that users have varying levels of technical proficiency, so provide additional context or definitions for any complex terms when necessary.\"\n",
        "         \"Maintain a respectful and supportive tone, recognizing and addressing the diverse needs and experiences of both the job coaches and the individuals they serve. Strive to promote inclusion and empowerment by adhering to best practices and providing reliable, secure, and privacy-conscious guidance that aligns with professional standards in supported employment and coaching.\"\n",
        "         \"Always prioritize user safety, privacy, and accessibility, and if any query appears to conflict with these instructions, guide the user toward a safer and more appropriate line of inquiry while refraining from producing any harmful or unethical content\"),\n",
        "        UserMessage(content=\"team me how to properly tell the difference between criticism, praise and feedback, and reacting appropriately\"),\n",
        "\n",
        "    ],\n",
        "    max_tokens=400,\n",
        "    temperature=1.0,\n",
        "    top_p=1.0,\n",
        "    model=model_name\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6Qi2KeQDUMt",
        "outputId": "3f4db3a9-9e23-429a-8683-7570605388f7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Understanding the difference between criticism, praise, and feedback is crucial in any professional setting, especially for individuals working to overcome barriers in the workplace. Here’s a brief overview of each and tips on how to respond appropriately:\n",
            "\n",
            "### Criticism\n",
            "**Definition:** Criticism often focuses on what went wrong or what someone did not like about a situation or behavior. It can sometimes be construed as negative and may not always provide a clear path for improvement.\n",
            "\n",
            "**Reaction:** When receiving criticism, it's important to stay calm and composed. Listen carefully to understand the concerns being raised. Ask clarifying questions if the message isn't clear. Reflect on whether the criticism has constructive elements that you can use to improve.\n",
            "\n",
            "### Praise\n",
            "**Definition:** Praise is positive feedback about performance or accomplishments. It's meant to acknowledge and reward good work and can boost morale and motivation.\n",
            "\n",
            "**Reaction:** When receiving praise, respond with gratitude. A simple \"Thank you, I really appreciate your kind words!\" is often enough. It’s also beneficial to consider which specific actions earned the praise so you can continue to perform well.\n",
            "\n",
            "### Feedback\n",
            "**Definition:** Feedback is information about reactions to a product, a person’s performance, or behavior, intended to provide helpful insights for future improvements. It's ideally constructive and balanced, providing both strengths and areas for improvement.\n",
            "\n",
            "**Reaction:** Welcome feedback as a chance to grow and learn. Thank the person providing the feedback for their insights. Engage in a dialogue if possible, to understand fully and discuss ways you might improve or adapt based on the feedback.\n",
            "\n",
            "### Tips for Reacting Appropriately:\n",
            "1. **Stay Open and Non-Defensive:** Regardless of the type of communication (criticism, praise, feedback), keeping an open mind helps you to absorb the message constructively.\n",
            "2. **Reflect:** Take a moment to think about what has been said before you respond. This helps in understanding the perspective of the other person and in formulating a thoughtful reply.\n",
            "3. **\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I am testing out document summarization!"
      ],
      "metadata": {
        "id": "SIIWJYysEyuJ"
      }
    },
    {
      "source": [
        "!pip install azure-ai-textanalytics"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "tVJtlh-LFiI0",
        "outputId": "9bd0339e-9dee-40d2-9285-905278fe5d34"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting azure-ai-textanalytics\n",
            "  Downloading azure_ai_textanalytics-5.3.0-py3-none-any.whl.metadata (82 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/82.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: azure-core<2.0.0,>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from azure-ai-textanalytics) (1.32.0)\n",
            "Collecting azure-common~=1.1 (from azure-ai-textanalytics)\n",
            "  Downloading azure_common-1.1.28-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: isodate<1.0.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from azure-ai-textanalytics) (0.7.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from azure-ai-textanalytics) (4.12.2)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2.32.3)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2025.1.31)\n",
            "Downloading azure_ai_textanalytics-5.3.0-py3-none-any.whl (298 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.6/298.6 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
            "Installing collected packages: azure-common, azure-ai-textanalytics\n",
            "Successfully installed azure-ai-textanalytics-5.3.0 azure-common-1.1.28\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "azure"
                ]
              },
              "id": "962fa248323f4d509e0000df0a707be1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# This example requires environment variables named \"AZURE_AI_KEY\" and \"ENDPOINT_TO_CALL_LANGUAGE_API\"\n",
        "key = \"4NVVBgTDzN81KPooWwjez6ItMgbWaGBjyZpAAdhFGTJtkfPsm1DGJQQJ99BCACYeBjFXJ3w3AAAAACOGASAx\" # Replace with your actual key\n",
        "endpoint = \"https://aitaskbuddy5634585164.cognitiveservices.azure.com/\" # Replace with your actual endpoint\n",
        "\n",
        "from azure.ai.textanalytics import TextAnalyticsClient\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "# Authenticate the client using your key and endpoint\n",
        "def authenticate_client():\n",
        "    ta_credential = AzureKeyCredential(key)\n",
        "    text_analytics_client = TextAnalyticsClient(\n",
        "            endpoint=endpoint,\n",
        "            credential=ta_credential)\n",
        "    return text_analytics_client\n",
        "\n",
        "client = authenticate_client()\n",
        "\n",
        "# Example method for summarizing text\n",
        "def sample_extractive_summarization(client):\n",
        "    from azure.core.credentials import AzureKeyCredential\n",
        "    from azure.ai.textanalytics import (\n",
        "        TextAnalyticsClient,\n",
        "        ExtractiveSummaryAction\n",
        "    )\n",
        "\n",
        "    document = [\n",
        "        \"Artificial intelligence is transforming the healthcare industry by improving diagnostics, automating administrative tasks, and personalizing patient treatment.\"\n",
        "        \"Machine learning models can analyze vast amounts of medical data to detect diseases earlier and with greater accuracy. Additionally, \"\n",
        "        \"AI-powered chatbots and virtual assistants help streamline patient interactions, reducing the workload on healthcare professionals. \"\n",
        "        \"However, challenges such as data privacy concerns and the need for regulatory compliance remain critical in the widespread adoption of AI in healthcare.\"\n",
        "\n",
        "    ]\n",
        "\n",
        "    poller = client.begin_analyze_actions(\n",
        "        document,\n",
        "        actions=[\n",
        "            ExtractiveSummaryAction(max_sentence_count=4)\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    document_results = poller.result()\n",
        "    for result in document_results:\n",
        "        extract_summary_result = result[0]  # first document, first result\n",
        "        if extract_summary_result.is_error:\n",
        "            print(\"...Is an error with code '{}' and message '{}'\".format(\n",
        "                extract_summary_result.code, extract_summary_result.message\n",
        "            ))\n",
        "        else:\n",
        "            print(\"Summary extracted: {}\".format(\n",
        "                \" \".join([sentence.text for sentence in extract_summary_result.sentences])))\n",
        "\n",
        "sample_extractive_summarization(client)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTgeIhMuE25m",
        "outputId": "04cd8b00-f037-4ab9-fff7-b634a479b659"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary extracted: Artificial intelligence is transforming the healthcare industry by improving diagnostics, automating administrative tasks, and personalizing patient treatment. Machine learning models can analyze vast amounts of medical data to detect diseases earlier and with greater accuracy. Additionally, AI-powered chatbots and virtual assistants help streamline patient interactions, reducing the workload on healthcare professionals. However, challenges such as data privacy concerns and the need for regulatory compliance remain critical in the widespread adoption of AI in healthcare.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing out Speech Capabilities\n"
      ],
      "metadata": {
        "id": "eW1nuNnbKgBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install azure-cognitiveservices-speech"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPqEHL8uMlmJ",
        "outputId": "3cefe445-ca46-4961-8ad9-247c8d9beb08"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting azure-cognitiveservices-speech\n",
            "  Downloading azure_cognitiveservices_speech-1.43.0-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Downloading azure_cognitiveservices_speech-1.43.0-py3-none-manylinux1_x86_64.whl (40.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: azure-cognitiveservices-speech\n",
            "Successfully installed azure-cognitiveservices-speech-1.43.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key = \"4NVVBgTDzN81KPooWwjez6ItMgbWaGBjyZpAAdhFGTJtkfPsm1DGJQQJ99BCACYeBjFXJ3w3AAAAACOGASAx\"\n",
        "endpoint = \"https://aitaskbuddy5634585164.cognitiveservices.azure.com/\""
      ],
      "metadata": {
        "id": "s2wK_EcAKjfR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"SPEECH_KEY\"] = \"4NVVBgTDzN81KPooWwjez6ItMgbWaGBjyZpAAdhFGTJtkfPsm1DGJQQJ99BCACYeBjFXJ3w3AAAAACOGASAx\"\n",
        "os.environ[\"SPEECH_REGION\"] = \"eastus\"\n"
      ],
      "metadata": {
        "id": "i_zSYRiaNW1g"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TESTING OUT SPEECH FOR A WAV FILE"
      ],
      "metadata": {
        "id": "RCzebXCpQMGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# This will open a file upload dialog.\n",
        "uploaded_files = files.upload()\n",
        "\n",
        "# List the uploaded files\n",
        "for filename in uploaded_files.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "        name=filename, length=len(uploaded_files[filename])))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "sH9EI6SOPU2d",
        "outputId": "ba3ca334-e8bd-479b-ab6e-2a0c6e9b2b43"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b5bffcfd-3ea9-4365-8c58-61342024ec7d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b5bffcfd-3ea9-4365-8c58-61342024ec7d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving cietta 4.m4a to cietta 4.m4a\n",
            "User uploaded file \"cietta 4.m4a\" with length 37546 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install ffmpeg -y\n",
        "!ffmpeg -i \"cietta 4.m4a\" -ar 16000 -ac 1 output.wav\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EE2yL46P8R8",
        "outputId": "0158558d-1077-4331-c055-d670996ccdcc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n",
            "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'cietta 4.m4a':\n",
            "  Metadata:\n",
            "    major_brand     : M4A \n",
            "    minor_version   : 0\n",
            "    compatible_brands: M4A isommp42\n",
            "    creation_time   : 2023-09-25T15:22:14.000000Z\n",
            "    title           : cietta 4\n",
            "    voice-memo-uuid : 613A9928-77A6-421E-B0E8-09BAAEC97BB1\n",
            "    encoder         : com.apple.VoiceMemos (iPhone Version 16.3.1 (Build 20D67))\n",
            "  Duration: 00:00:03.90, start: 0.000000, bitrate: 77 kb/s\n",
            "  Stream #0:0(und): Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, mono, fltp, 65 kb/s (default)\n",
            "    Metadata:\n",
            "      creation_time   : 2023-09-25T15:22:14.000000Z\n",
            "      handler_name    : Core Media Audio\n",
            "      vendor_id       : [0][0][0][0]\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (aac (native) -> pcm_s16le (native))\n",
            "Press [q] to stop, [?] for help\n",
            "Output #0, wav, to 'output.wav':\n",
            "  Metadata:\n",
            "    major_brand     : M4A \n",
            "    minor_version   : 0\n",
            "    compatible_brands: M4A isommp42\n",
            "    voice-memo-uuid : 613A9928-77A6-421E-B0E8-09BAAEC97BB1\n",
            "    INAM            : cietta 4\n",
            "    ISFT            : Lavf58.76.100\n",
            "  Stream #0:0(und): Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s (default)\n",
            "    Metadata:\n",
            "      creation_time   : 2023-09-25T15:22:14.000000Z\n",
            "      handler_name    : Core Media Audio\n",
            "      vendor_id       : [0][0][0][0]\n",
            "      encoder         : Lavc58.134.100 pcm_s16le\n",
            "size=     123kB time=00:00:03.94 bitrate= 256.3kbits/s speed= 197x    \n",
            "video:0kB audio:123kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.076013%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import azure.cognitiveservices.speech as speechsdk\n",
        "\n",
        "def recognize_from_audio_file(audio_filename):\n",
        "    # Ensure your environment variables are set for Speech Key and Region\n",
        "    speech_config = speechsdk.SpeechConfig(subscription=os.environ.get('SPEECH_KEY'),\n",
        "                                           region=os.environ.get('SPEECH_REGION'))\n",
        "    speech_config.speech_recognition_language = \"en-US\"\n",
        "\n",
        "    # Use the uploaded audio file instead of the microphone\n",
        "    audio_config = speechsdk.audio.AudioConfig(filename=audio_filename)\n",
        "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
        "\n",
        "    print(\"Recognizing speech from audio file...\")\n",
        "    speech_recognition_result = speech_recognizer.recognize_once_async().get()\n",
        "\n",
        "    if speech_recognition_result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
        "        print(\"Recognized: {}\".format(speech_recognition_result.text))\n",
        "    elif speech_recognition_result.reason == speechsdk.ResultReason.NoMatch:\n",
        "        print(\"No speech could be recognized: {}\".format(speech_recognition_result.no_match_details))\n",
        "    elif speech_recognition_result.reason == speechsdk.ResultReason.Canceled:\n",
        "        cancellation_details = speech_recognition_result.cancellation_details\n",
        "        print(\"Speech Recognition canceled: {}\".format(cancellation_details.reason))\n",
        "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
        "            print(\"Error details: {}\".format(cancellation_details.error_details))\n",
        "            print(\"Did you set the speech resource key and region values?\")\n",
        "\n",
        "# Replace the filename below with the name of your uploaded file.\n",
        "audio_filename = \"output.wav\"\n",
        "recognize_from_audio_file(audio_filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WD7v-PaPq9V",
        "outputId": "74f570de-ad04-42ff-ee3f-d414d5b6ce9f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recognizing speech from audio file...\n",
            "Recognized: However, things turned around for her when she started getting more involved on campus.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TESTING OUT SPEECH FOR A MICROPHONE\n",
        "\n",
        "*   The code curently gives an error because i am running colab online and it does not have access to a microphone\n",
        "\n"
      ],
      "metadata": {
        "id": "GsMY1JoiQT4n"
      }
    },
    {
      "source": [
        "import os\n",
        "import azure.cognitiveservices.speech as speechsdk\n",
        "\n",
        "def recognize_from_microphone():\n",
        "    # This example requires environment variables named \"SPEECH_KEY\" and \"SPEECH_REGION\"\n",
        "    speech_config = speechsdk.SpeechConfig(subscription=os.environ.get('SPEECH_KEY'), region=os.environ.get('SPEECH_REGION'))\n",
        "    speech_config.speech_recognition_language=\"en-US\"\n",
        "\n",
        "    audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
        "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
        "\n",
        "    print(\"Speak into your microphone.\")\n",
        "    speech_recognition_result = speech_recognizer.recognize_once_async().get()\n",
        "\n",
        "    if speech_recognition_result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
        "        print(\"Recognized: {}\".format(speech_recognition_result.text))\n",
        "    elif speech_recognition_result.reason == speechsdk.ResultReason.NoMatch:\n",
        "        print(\"No speech could be recognized: {}\".format(speech_recognition_result.no_match_details))\n",
        "    elif speech_recognition_result.reason == speechsdk.ResultReason.Canceled:\n",
        "        cancellation_details = speech_recognition_result.cancellation_details\n",
        "        print(\"Speech Recognition canceled: {}\".format(cancellation_details.reason))\n",
        "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
        "            print(\"Error details: {}\".format(cancellation_details.error_details))\n",
        "            print(\"Did you set the speech resource key and region values?\")\n",
        "\n",
        "recognize_from_microphone()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-61MLSm7W8l6",
        "outputId": "21069782-386e-4ca6-b5c3-9be413786a1d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Exception with error code: \n[CALL STACK BEGIN]\n\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.extension.audio.sys.so(+0x10168) [0x7daea0034168]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0xa75e6) [0x7dae8a2b95e6]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1898fb) [0x7dae8a39b8fb]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0xe2650) [0x7dae8a2f4650]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1940f2) [0x7dae8a3a60f2]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0xa75e6) [0x7dae8a2b95e6]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1898fb) [0x7dae8a39b8fb]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0xe02dd) [0x7dae8a2f22dd]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0xe393b) [0x7dae8a2f593b]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x149fcb) [0x7dae8a35bfcb]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x149fcb) [0x7dae8a35bfcb]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x10a663) [0x7dae8a31c663]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x13d5f6) [0x7dae8a34f5f6]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x99557) [0x7dae8a2ab557]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x13cb2b) [0x7dae8a34eb2b]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x91e35) [0x7dae8a2a3e35]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(recognizer_create_speech_recognizer_from_config+0x114) [0x7dae8a3e8953]\n[CALL STACK END]\n\nException with an error code: 0xe (SPXERR_MIC_NOT_AVAILABLE)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-8cb8e1b73f1f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Did you set the speech resource key and region values?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mrecognize_from_microphone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-8cb8e1b73f1f>\u001b[0m in \u001b[0;36mrecognize_from_microphone\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0maudio_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspeechsdk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAudioConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_default_microphone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mspeech_recognizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspeechsdk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpeechRecognizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeech_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspeech_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maudio_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Speak into your microphone.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/speech.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, speech_config, audio_config, language, source_language_config, auto_detect_source_language_config)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0maudio_config_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0maudio_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msource_language_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mauto_detect_source_language_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             _call_hr_fn(\n\u001b[0m\u001b[1;32m   1008\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_sdk_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecognizer_create_speech_recognizer_from_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m                 *[ctypes.byref(handle), speech_config._handle, audio_config_handle])\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/interop.py\u001b[0m in \u001b[0;36m_call_hr_fn\u001b[0;34m(fn, *args)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_spx_hr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mhr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0m_raise_if_failed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/interop.py\u001b[0m in \u001b[0;36m_raise_if_failed\u001b[0;34m(hr)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_if_failed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_spx_hr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhr\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0m__try_get_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_spx_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/interop.py\u001b[0m in \u001b[0;36m__try_get_error\u001b[0;34m(error_handle)\u001b[0m\n\u001b[1;32m     48\u001b[0m     )\n\u001b[1;32m     49\u001b[0m     \u001b[0m_sdk_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_release\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Exception with error code: \n[CALL STACK BEGIN]\n\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.extension.audio.sys.so(+0x10168) [0x7daea0034168]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0xa75e6) [0x7dae8a2b95e6]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1898fb) [0x7dae8a39b8fb]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0xe2650) [0x7dae8a2f4650]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1940f2) [0x7dae8a3a60f2]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0xa75e6) [0x7dae8a2b95e6]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1898fb) [0x7dae8a39b8fb]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0xe02dd) [0x7dae8a2f22dd]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0xe393b) [0x7dae8a2f593b]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x149fcb) [0x7dae8a35bfcb]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x149fcb) [0x7dae8a35bfcb]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x10a663) [0x7dae8a31c663]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x13d5f6) [0x7dae8a34f5f6]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x99557) [0x7dae8a2ab557]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x13cb2b) [0x7dae8a34eb2b]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x91e35) [0x7dae8a2a3e35]\n/usr/local/lib/python3.11/dist-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(recognizer_create_speech_recognizer_from_config+0x114) [0x7dae8a3e8953]\n[CALL STACK END]\n\nException with an error code: 0xe (SPXERR_MIC_NOT_AVAILABLE)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DOCUMENT INTELLIGENCE\n",
        "\n",
        "\n",
        "reads documents and extracrats key information identifying onformation from forms"
      ],
      "metadata": {
        "id": "BxmyOhkFgKBA"
      }
    },
    {
      "source": [
        "!pip install azure-core\n",
        "!pip install azure-ai-formrecognizer\n",
        "!pip install azure-cognitiveservices-speech\n",
        "!pip install azure-identity"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "a6RKr64Rgy8C",
        "outputId": "aa0bd638-9609-4042-f10d-5af5c1340c63"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting azure-core\n",
            "  Downloading azure_core-1.32.0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from azure-core) (2.32.3)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from azure-core) (1.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from azure-core) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core) (2025.1.31)\n",
            "Downloading azure_core-1.32.0-py3-none-any.whl (198 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/198.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.9/198.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: azure-core\n",
            "Successfully installed azure-core-1.32.0\n",
            "Collecting azure-ai-formrecognizer\n",
            "  Downloading azure_ai_formrecognizer-3.3.3-py3-none-any.whl.metadata (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.6/64.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: azure-core>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from azure-ai-formrecognizer) (1.32.0)\n",
            "Collecting msrest>=0.6.21 (from azure-ai-formrecognizer)\n",
            "  Downloading msrest-0.7.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting azure-common>=1.1 (from azure-ai-formrecognizer)\n",
            "  Downloading azure_common-1.1.28-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from azure-ai-formrecognizer) (4.12.2)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.23.0->azure-ai-formrecognizer) (2.32.3)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.23.0->azure-ai-formrecognizer) (1.17.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from msrest>=0.6.21->azure-ai-formrecognizer) (2025.1.31)\n",
            "Collecting isodate>=0.6.0 (from msrest>=0.6.21->azure-ai-formrecognizer)\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from msrest>=0.6.21->azure-ai-formrecognizer) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.23.0->azure-ai-formrecognizer) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.23.0->azure-ai-formrecognizer) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.23.0->azure-ai-formrecognizer) (2.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.21->azure-ai-formrecognizer) (3.2.2)\n",
            "Downloading azure_ai_formrecognizer-3.3.3-py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.4/301.4 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
            "Downloading msrest-0.7.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: azure-common, isodate, msrest, azure-ai-formrecognizer\n",
            "Successfully installed azure-ai-formrecognizer-3.3.3 azure-common-1.1.28 isodate-0.7.2 msrest-0.7.1\n",
            "Collecting azure-cognitiveservices-speech\n",
            "  Downloading azure_cognitiveservices_speech-1.43.0-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Downloading azure_cognitiveservices_speech-1.43.0-py3-none-manylinux1_x86_64.whl (40.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: azure-cognitiveservices-speech\n",
            "Successfully installed azure-cognitiveservices-speech-1.43.0\n",
            "Collecting azure-identity\n",
            "  Downloading azure_identity-1.21.0-py3-none-any.whl.metadata (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: azure-core>=1.31.0 in /usr/local/lib/python3.11/dist-packages (from azure-identity) (1.32.0)\n",
            "Requirement already satisfied: cryptography>=2.5 in /usr/local/lib/python3.11/dist-packages (from azure-identity) (43.0.3)\n",
            "Collecting msal>=1.30.0 (from azure-identity)\n",
            "  Downloading msal-1.32.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting msal-extensions>=1.2.0 (from azure-identity)\n",
            "  Downloading msal_extensions-1.3.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from azure-identity) (4.12.2)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.31.0->azure-identity) (2.32.3)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.31.0->azure-identity) (1.17.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=2.5->azure-identity) (1.17.1)\n",
            "Requirement already satisfied: PyJWT<3,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity) (2.10.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=2.5->azure-identity) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity) (2025.1.31)\n",
            "Downloading azure_identity-1.21.0-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.2/189.2 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msal-1.32.0-py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.7/114.7 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msal_extensions-1.3.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: msal, msal-extensions, azure-identity\n",
            "Successfully installed azure-identity-1.21.0 msal-1.32.0 msal-extensions-1.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extrating from a document with a document link"
      ],
      "metadata": {
        "id": "BwJ9BXZPjSGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
        "\n",
        "endpoint = \"https://documentintenlligenceaitaskbuddy.cognitiveservices.azure.com/\"\n",
        "key = \"1892579ceae345a9ae466c972f3f7cd4\"\n",
        "\n",
        "# sample document\n",
        "formUrl = \"https://raw.githubusercontent.com/Azure-Samples/cognitive-services-REST-api-samples/master/curl/form-recognizer/sample-layout.pdf\"\n",
        "\n",
        "document_analysis_client = DocumentAnalysisClient(\n",
        "        endpoint=endpoint, credential=AzureKeyCredential(key)\n",
        "    )\n",
        "\n",
        "poller = document_analysis_client.begin_analyze_document_from_url(\"prebuilt-document\", formUrl)\n",
        "result = poller.result()\n",
        "\n",
        "print(\"----Key-value pairs found in document----\")\n",
        "for kv_pair in result.key_value_pairs:\n",
        "    if kv_pair.key and kv_pair.value:\n",
        "        print(\"Key '{}': Value: '{}'\".format(kv_pair.key.content, kv_pair.value.content))\n",
        "    else:\n",
        "        print(\"Key '{}': Value:\".format(kv_pair.key.content))\n",
        "\n",
        "print(\"----------------------------------------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnFrORT3gJlt",
        "outputId": "bac0ee0a-ab83-444d-9a8b-f8caa5748936"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----Key-value pairs found in document----\n",
            "Key 'QUARTERLY REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934': Value: ':selected:'\n",
            "Key 'For the Quarterly Period Ended': Value: 'March 31, 2020'\n",
            "Key 'TRANSITION REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934': Value: ':unselected:'\n",
            "Key 'Transition Period From': Value:\n",
            "Key 'Commission File Number': Value: '001-37845'\n",
            "Key '(STATE': Value: 'WASHINGTON'\n",
            "Key '(I.R.S. ID)': Value: '91-1144442'\n",
            "Key 'Securities registered pursuant to Section 12(g) of the Act:': Value: 'NONE'\n",
            "Key 'Yes': Value: ':selected:'\n",
            "Key 'No': Value: ':unselected:'\n",
            "Key 'Yes': Value: ':selected:'\n",
            "Key 'No': Value: ':unselected:'\n",
            "Key 'Large accelerated filer': Value: ':selected:'\n",
            "Key 'Accelerated filer': Value: ':unselected:'\n",
            "Key 'Non-accelerated filer': Value: ':unselected:'\n",
            "Key 'Emerging growth company': Value: ':unselected:'\n",
            "Key 'If an emerging growth company, indicate by check mark if the registrant has elected not to use the extended transition period for complying with any new or revised financial accounting standards provided pursuant to Section 13(a) of the Exchange Act.': Value: ':unselected:'\n",
            "Key 'Yes': Value: ':unselected:'\n",
            "Key 'No': Value: ':selected:'\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXTRACTING FROM AN UPLOADED DOCUMENT"
      ],
      "metadata": {
        "id": "u5iu2HOJjXDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.core.credentials import AzureKeyCredential\n",
        "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
        "from google.colab import files\n",
        "\n",
        "# Azure Form Recognizer credentials\n",
        "endpoint = \"https://documentintenlligenceaitaskbuddy.cognitiveservices.azure.com/\"\n",
        "key = \"1892579ceae345a9ae466c972f3f7cd4\"\n",
        "\n",
        "# Create the DocumentAnalysisClient\n",
        "document_analysis_client = DocumentAnalysisClient(\n",
        "    endpoint=endpoint, credential=AzureKeyCredential(key)\n",
        ")\n",
        "\n",
        "print(\"Please upload your document file (e.g., a PDF):\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    # Get the name of the first uploaded file\n",
        "    file_name = next(iter(uploaded))\n",
        "    print(\"Using uploaded file:\", file_name)\n",
        "\n",
        "    # Open the file in binary read mode and analyze it\n",
        "    with open(file_name, \"rb\") as f:\n",
        "        poller = document_analysis_client.begin_analyze_document(\"prebuilt-document\", document=f)\n",
        "        result = poller.result()\n",
        "\n",
        "    print(\"----Key-value pairs found in document----\")\n",
        "    for kv_pair in result.key_value_pairs:\n",
        "        if kv_pair.key and kv_pair.value:\n",
        "            print(\"Key '{}': Value: '{}'\".format(kv_pair.key.content, kv_pair.value.content))\n",
        "        else:\n",
        "            print(\"Key '{}': Value:\".format(kv_pair.key.content))\n",
        "    print(\"----------------------------------------\")\n",
        "else:\n",
        "    print(\"No file was uploaded.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "oL5YTVddjeeE",
        "outputId": "e97641fe-3e28-4ead-e2df-2e3887323f5d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your document file (e.g., a PDF):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-27d30a81-452d-4310-a46d-34848edb4a5d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-27d30a81-452d-4310-a46d-34848edb4a5d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving agreement and release form (1).pdf to agreement and release form (1).pdf\n",
            "Using uploaded file: agreement and release form (1).pdf\n",
            "----Key-value pairs found in document----\n",
            "Key 'Name': Value: 'Agness Lungu'\n",
            "Key 'Program': Value: 'Ghana Tech Trek'\n",
            "Key 'Student's initials': Value: 'AL'\n",
            "Key 'Parent's initials (see below)': Value:\n",
            "Key 'Student's Signature': Value:\n",
            "Key 'Date': Value: '03/17/2025'\n",
            "Key 'Name (printed)': Value: 'AGNESS LUNGU'\n",
            "Key 'Program': Value: 'Ghana Tech Trek'\n",
            "Key 'Parent/Guardian's Signature': Value:\n",
            "Key 'Date': Value:\n",
            "----------------------------------------\n"
          ]
        }
      ]
    }
  ]
}